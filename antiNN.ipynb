{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import implicit\n",
    "import lightgbm as lgb\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FM_FTRL, FTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename, array):\n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>QueryTS</th>\n",
       "      <th>ProductID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77043f410ba7dc8dced745db94a1fcba</td>\n",
       "      <td>02142c5dccec42262a1e88dc8416f5b7</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>634e6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c5ad385cba5ac43b0263f3d804c8f823</td>\n",
       "      <td>8d7a1059ef78672047e45cf1b84d6276</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>7acab3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c5ad385cba5ac43b0263f3d804c8f823</td>\n",
       "      <td>8d7a1059ef78672047e45cf1b84d6276</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>7acab3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75d7a7507ac89b33191b18d56af7544b</td>\n",
       "      <td>8d7a1059ef78672047e45cf1b84d6276</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>7acab3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75d7a7507ac89b33191b18d56af7544b</td>\n",
       "      <td>8d7a1059ef78672047e45cf1b84d6276</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>7acab3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FileID                        CustomerID  \\\n",
       "0  77043f410ba7dc8dced745db94a1fcba  02142c5dccec42262a1e88dc8416f5b7   \n",
       "1  c5ad385cba5ac43b0263f3d804c8f823  8d7a1059ef78672047e45cf1b84d6276   \n",
       "2  c5ad385cba5ac43b0263f3d804c8f823  8d7a1059ef78672047e45cf1b84d6276   \n",
       "3  75d7a7507ac89b33191b18d56af7544b  8d7a1059ef78672047e45cf1b84d6276   \n",
       "4  75d7a7507ac89b33191b18d56af7544b  8d7a1059ef78672047e45cf1b84d6276   \n",
       "\n",
       "     QueryTS ProductID  \n",
       "0 2017-03-01    634e6b  \n",
       "1 2017-03-01    7acab3  \n",
       "2 2017-03-01    7acab3  \n",
       "3 2017-03-01    7acab3  \n",
       "4 2017-03-01    7acab3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = '/disk/Tbrain/'\n",
    "\n",
    "train_set = pd.read_csv(dir_path+'training-set.csv', header=None, names=['FileID', 'Target'])\n",
    "test_set = pd.read_csv(dir_path+'testing-set.csv', header=None, names=['FileID', 'Target'])\n",
    "train_ex = pd.read_table(dir_path+'exception/exception_train.txt', header=None, names=['FileID'])\n",
    "test_ex = pd.read_table(dir_path+'exception/exception_testing.txt', header=None, names=['FileID'])\n",
    "\n",
    "train_set = train_set.loc[~train_set['FileID'].isin(train_ex)]\n",
    "test_set = test_set.loc[~test_set['FileID'].isin(test_ex)]\n",
    "\n",
    "\n",
    "log_data = pd.read_csv(dir_path+'log_data.csv')\n",
    "log_data['QueryTS'] = pd.to_datetime(log_data['QueryTS'], format='%Y-%m-%d %H:%M:%S')\n",
    "log_data.sort_values(['QueryTS'], ascending=True, na_position='first', inplace=True)\n",
    "log_data.reset_index(drop=True, inplace=True)\n",
    "log_data['ProductID'] = log_data['ProductID'].astype(str)\n",
    "print('Load data complete.')\n",
    "\n",
    "del train_ex, test_ex\n",
    "gc.collect()\n",
    "\n",
    "log_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e2398b12121a85166fed5fe2a3da</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001fe8dce14ce099aa6ca8ea5026ea7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027f50019000accc492e5684efc818</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00028c9da3573ec50db74b44310ae507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003dc8130969abe688cadf5f14ea19f</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FileID  Target\n",
       "0  0000e2398b12121a85166fed5fe2a3da     0.0\n",
       "1  0001fe8dce14ce099aa6ca8ea5026ea7     0.0\n",
       "2  00027f50019000accc492e5684efc818     0.0\n",
       "3  00028c9da3573ec50db74b44310ae507     0.0\n",
       "4  0003dc8130969abe688cadf5f14ea19f     0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([train_set, test_set],axis=0)\n",
    "data['Target'].replace(0.5, np.nan, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic Features\n",
    "CAUTION!! Related with `Target`, take care with dealing overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e2398b12121a85166fed5fe2a3da</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001fe8dce14ce099aa6ca8ea5026ea7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027f50019000accc492e5684efc818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00028c9da3573ec50db74b44310ae507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003dc8130969abe688cadf5f14ea19f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FileID  Target  Fold\n",
       "0  0000e2398b12121a85166fed5fe2a3da     0.0   0.0\n",
       "1  0001fe8dce14ce099aa6ca8ea5026ea7     0.0   0.0\n",
       "2  00027f50019000accc492e5684efc818     0.0   0.0\n",
       "3  00028c9da3573ec50db74b44310ae507     0.0   0.0\n",
       "4  0003dc8130969abe688cadf5f14ea19f     0.0   0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([train_set, test_set],axis=0)\n",
    "data['Target'].replace(0.5, np.nan, inplace=True)\n",
    "nrow_train = len(train_set)\n",
    "\n",
    "# Given fold number for each FileID (Need for feature engineering with y, prevent for overfitting)\n",
    "n_folds = 10\n",
    "skf = StratifiedKFold(n_splits=n_folds, random_state=5566)\n",
    "data['Fold'] = np.nan\n",
    "for f, (_, valid_idx) in enumerate(skf.split(data['FileID'].iloc[:nrow_train], data['Target'].iloc[:nrow_train])):\n",
    "    data['Fold'].iloc[valid_idx] = f\n",
    "\n",
    "log_data = pd.merge(log_data, data, on='FileID', how='left')    \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cust'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'CustomerID'[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by CustomerID completed.\n"
     ]
    }
   ],
   "source": [
    "CustomerID_y = log_data[(log_data['Fold'] != 0) & (log_data['Fold'].notnull())].groupby('CustomerID')['Target'].agg({'TargetCust_mean': np.mean, \n",
    "                                                                                                                     'TargetCust_std': np.std, \n",
    "                                                                                                                     'TargetCust_sum': np.sum}).reset_index()\n",
    "CustomerID_y = CustomerID_y[CustomerID_y['CustomerID'].isin(list(log_data[log_data['Fold'] == 0]['CustomerID']))]\n",
    "CustomerID_y['Fold'] = 0\n",
    "\n",
    "for fold in range(1, n_folds):\n",
    "    tmp = log_data[(log_data['Fold'] != fold) & (log_data['Fold'].notnull())].groupby('CustomerID')['Target'].agg({'TargetCust_mean': np.mean, \n",
    "                                                                                                                   'TargetCust_std': np.std, \n",
    "                                                                                                                   'TargetCust_sum': np.sum}).reset_index()\n",
    "    tmp = tmp[tmp['CustomerID'].isin(list(log_data[log_data['Fold'] == fold]['CustomerID']))]\n",
    "    tmp['Fold'] = fold\n",
    "    CustomerID_y = CustomerID_y.append(tmp)\n",
    "\n",
    "tmp = log_data[log_data['Fold'].notnull()].groupby('CustomerID')['Target'].agg({'TargetCust_mean': np.mean, \n",
    "                                                                                'TargetCust_std': np.std, \n",
    "                                                                                'TargetCust_sum': np.sum}).reset_index()\n",
    "tmp = tmp[tmp['CustomerID'].isin(list(log_data[log_data['Fold'].isnull()]['CustomerID']))]\n",
    "tmp['Fold'] = np.nan\n",
    "CustomerID_y = CustomerID_y.append(tmp)\n",
    "print('Group by CustomerID completed.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by ProductID completed.\n"
     ]
    }
   ],
   "source": [
    "ProductID_y = log_data[(log_data['Fold'] != 0) & (log_data['Fold'].notnull())].groupby('ProductID')['Target'].agg({'TargetProd_mean': np.mean, \n",
    "                                                                                                                   'TargetProd_std': np.std, \n",
    "                                                                                                                   'TargetProd_sum': np.sum}).reset_index()\n",
    "ProductID_y = ProductID_y[ProductID_y['ProductID'].isin(list(log_data[log_data['Fold'] == 0]['ProductID']))]\n",
    "ProductID_y['Fold'] = 0\n",
    "\n",
    "for fold in range(1, n_folds):\n",
    "    tmp = log_data[(log_data['Fold'] != fold) & (log_data['Fold'].notnull())].groupby('ProductID')['Target'].agg({'TargetProd_mean': np.mean, \n",
    "                                                                                                                  'TargetProd_std': np.std, \n",
    "                                                                                                                  'TargetProd_sum': np.sum}).reset_index()\n",
    "    tmp = tmp[tmp['ProductID'].isin(list(log_data[log_data['Fold'] == fold]['ProductID']))]\n",
    "    tmp['Fold'] = fold\n",
    "    ProductID_y = ProductID_y.append(tmp)\n",
    "\n",
    "tmp = log_data[log_data['Fold'].notnull()].groupby('ProductID')['Target'].agg({'TargetProd_mean': np.mean, \n",
    "                                                                               'TargetProd_std': np.std, \n",
    "                                                                               'TargetProd_sum': np.sum}).reset_index()\n",
    "tmp = tmp[tmp['ProductID'].isin(list(log_data[log_data['Fold'].isnull()]['ProductID']))]\n",
    "tmp['Fold'] = np.nan\n",
    "ProductID_y = ProductID_y.append(tmp)\n",
    "print('Group by ProductID completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data = pd.merge(log_data, CustomerID_y, on=['CustomerID', 'Fold'], how='left')\n",
    "log_data = pd.merge(log_data, ProductID_y, on=['ProductID', 'Fold'], how='left')\n",
    "\n",
    "# for col in ['TargetCust_sum', 'TargetProd_sum']:\n",
    "#     log_data[col] = (log_data[col]-np.mean(log_data[col])) / (np.std(log_data[col])+np.mean(log_data[col])+1)\n",
    "\n",
    "# for col in ['TargetCust_mean', 'TargetCust_std', 'TargetCust_sum', 'TargetProd_mean', 'TargetProd_std', 'TargetProd_sum']:\n",
    "#     log_data[col].fillna(-1, inplace=True)\n",
    "\n",
    "del CustomerID_y, ProductID_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features related with`Target`: (81894, 55)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1499"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_feature = pd.DataFrame()\n",
    "cols_to_use = ['TargetCust_mean', 'TargetCust_std', 'TargetCust_sum', 'TargetProd_mean', 'TargetProd_std', \n",
    "               'TargetProd_sum']\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    tmp_all = log_data.loc[log_data['Fold'] == fold].groupby('FileID')[cols_to_use].agg({'_mean': np.mean, '_std': np.std, \n",
    "                                                                                         '_min': np.min, '_max': np.max, \n",
    "                                                                                         '_median': np.median, \n",
    "                                                                                         '_10per': lambda x: np.percentile(x, q=10), \n",
    "                                                                                         '_25per': lambda x: np.percentile(x, q=25),\n",
    "                                                                                         '_75per': lambda x: np.percentile(x, q=75),\n",
    "                                                                                         '_90per': lambda x: np.percentile(x, q=90)})\n",
    "    tmp_4 = pd.DataFrame()\n",
    "    for col in ['_mean', '_std', '_min', '_median', '_max', '_10per', '_25per', '_75per', '_90per']:\n",
    "        tmp = tmp_all[col]\n",
    "        tmp.columns = [sub_col+col for sub_col in tmp.columns]\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp_4 = pd.concat([tmp_4, tmp], axis=1)\n",
    "        \n",
    "    target_feature = pd.concat([target_feature, tmp_4], axis=0)\n",
    "\n",
    "tmp_all = log_data.loc[log_data['Fold'].isnull()].groupby('FileID')[cols_to_use].agg({'_mean': np.mean, '_std': np.std, \n",
    "                                                                                      '_min': np.min, '_max': np.max,\n",
    "                                                                                      '_median': np.median, \n",
    "                                                                                      '_10per': lambda x: np.percentile(x, q=10), \n",
    "                                                                                      '_25per': lambda x: np.percentile(x, q=25),\n",
    "                                                                                      '_75per': lambda x: np.percentile(x, q=75),\n",
    "                                                                                      '_90per': lambda x: np.percentile(x, q=90)})\n",
    "tmp_4 = pd.DataFrame()\n",
    "for col in ['_mean', '_std', '_min', '_median', '_max', '_10per', '_25per', '_75per', '_90per']:\n",
    "    tmp = tmp_all[col]\n",
    "    tmp.columns = [sub_col+col for sub_col in tmp.columns]\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp_4 = pd.concat([tmp_4, tmp], axis=1)\n",
    "\n",
    "target_feature = pd.concat([target_feature, tmp_4], axis=0)\n",
    "\n",
    "tmp = pd.Series(target_feature.iloc[:,0], name='FileID').to_frame()\n",
    "target_feature.drop(['FileID'], axis=True, inplace=True)\n",
    "target_feature = pd.concat([tmp, target_feature], axis=1)\n",
    "target_feature.fillna(-1, inplace=True)\n",
    "print('Shape of features related with`Target`: {0}'.format(target_feature.shape))\n",
    "\n",
    "data = pd.merge(data, target_feature, on='FileID', how='left')\n",
    "\n",
    "# Drop columns only useful for magic features.\n",
    "log_data.drop(log_data.columns.tolist()[5:], axis=1, inplace=True)\n",
    "del target_feature\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data['FileID'] = log_data['FileID'].astype('category', categories=data['FileID'].values)\n",
    "log_data['CustomerID'] = log_data['CustomerID'].astype('category')\n",
    "\n",
    "row = log_data['FileID'].cat.codes\n",
    "col = log_data['CustomerID'].cat.codes\n",
    "\n",
    "dtm = csr_matrix((np.ones(len(log_data)), (row, col)))\n",
    "dtm = dtm[:, np.where(dtm.getnnz(axis=0) > 1)[0]]\n",
    "print('Shape of DTM: {0}'.format(dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Alternating Least Square to reduce dimension (latent factor)\n",
    "als = implicit.als.AlternatingLeastSquares(factors=350)\n",
    "als.fit(dtm)\n",
    "X_als = csr_matrix(als.item_factors)\n",
    "\n",
    "del dtm\n",
    "gc.collect()\n",
    "\n",
    "print('Extract ALS latent factor completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Time Series of CustomerID and ProductID\n",
    "log_data['CustomerID_le'] = row\n",
    "log_data['ProductID_le'] = col\n",
    "# Sequence of CustomerID\n",
    "tmp = log_data.groupby('FileID')['CustomerID_le'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "# Sequence of ProductID\n",
    "tmp = log_data.groupby('FileID')['ProductID_le'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "log_data.drop(['CustomerID_le', 'ProductID_le'], axis=1, inplace=True)\n",
    "print('Sequentail lebal encoding completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ngram(q, n_gram_max):\n",
    "        ngrams = []\n",
    "        for n in range(2, n_gram_max+1):\n",
    "            for w_index in range(len(q)-n+1):\n",
    "                ngrams.append(''.join(q[w_index:w_index+n]))\n",
    "        return q + ngrams\n",
    "\n",
    "def list2str(text):\n",
    "    return u' '.join([str(x) for x in text])\n",
    "\n",
    "\n",
    "wb = wordbatch.WordBatch(list2str, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0], \n",
    "                                                        \"hash_size\": 2 ** 29, \"norm\": \"l2\", \"tf\": 'binary',\n",
    "                                                        \"idf\": None}), procs=8)\n",
    "wb.dictionary_freeze= True\n",
    "X_cust = wb.fit_transform(data['CustomerID_le'])\n",
    "X_cust = X_cust[:, np.where(X_cust.getnnz(axis=0) > 1)[0]]\n",
    "print('Shape of X_cust: {0}'.format(X_cust.shape))\n",
    "del wb\n",
    "\n",
    "\n",
    "print('Vectorize `CustomerID` completed.')\n",
    "\n",
    "wb = wordbatch.WordBatch(list2str, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0], \n",
    "                                                        \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                                        \"idf\": None}), procs=8)\n",
    "wb.dictionary_freeze= True\n",
    "X_prod = wb.fit_transform(data['ProductID_le'])\n",
    "X_prod = X_prod[:, np.where(X_prod.getnnz(axis=0) > 10)[0]]\n",
    "print('Shape of X_prod: {0}'.format(X_prod.shape))\n",
    "# Fit ALS\n",
    "als = implicit.als.AlternatingLeastSquares(factors=100)\n",
    "als.fit(X_prod)\n",
    "X_prod = csr_matrix(als.item_factors)\n",
    "del wb, als\n",
    "\n",
    "\n",
    "data.drop(['CustomerID_le', 'ProductID_le'], axis=1, inplace=True)\n",
    "\n",
    "print('Vectorize `ProductID` completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate list of time delta sequence complete.\n"
     ]
    }
   ],
   "source": [
    "# Sequence of FileDiff\n",
    "log_data['FileDiff'] = log_data['QueryTS'].diff().dt.total_seconds()\n",
    "log_data['FileDiffgFile'] = log_data.groupby('FileID')['QueryTS'].diff().dt.total_seconds()\n",
    "log_data['FileDiffgCust'] = log_data.groupby('CustomerID')['QueryTS'].diff().dt.total_seconds()\n",
    "log_data['FileDiffgProd'] = log_data.groupby('ProductID')['QueryTS'].diff().dt.total_seconds()\n",
    "\n",
    "# FileDiff\n",
    "tmp = log_data.groupby('FileID')['FileDiff'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "# FileDiffgFile\n",
    "tmp = log_data.groupby('FileID')['FileDiffgFile'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "# FileDiffgCust\n",
    "tmp = log_data.groupby('FileID')['FileDiffgCust'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "# FileDiffgProd\n",
    "tmp = log_data.groupby('FileID')['FileDiffgProd'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "print('Generate list of time delta sequence complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = wordbatch.WordBatch(list2str, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0], \n",
    "                                                        \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 'binary',\n",
    "                                                        \"idf\": None}), procs=24)\n",
    "wb.dictionary_freeze= True\n",
    "X_diff = wb.fit_transform(data['FileDiff'])\n",
    "X_diff = X_diff[:, np.where(X_diff.getnnz(axis=0) > 1)[0]]\n",
    "print('Shape of X_cust: {0}'.format(X_diff.shape))\n",
    "del wb\n",
    "\n",
    "wb = wordbatch.WordBatch(list2str, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0], \n",
    "                                                        \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 'binary',\n",
    "                                                        \"idf\": None}), procs=24)\n",
    "wb.dictionary_freeze= True\n",
    "tmp = wb.fit_transform(data['FileDiffgFile'])\n",
    "tmp = tmp[:, np.where(tmp.getnnz(axis=0) > 5)[0]]\n",
    "print('Shape of X_prod: {0}'.format(tmp.shape))\n",
    "# Fit ALS\n",
    "als = implicit.als.AlternatingLeastSquares(factors=300)\n",
    "als.fit(tmp)\n",
    "tmp = csr_matrix(als.item_factors)\n",
    "\n",
    "del wb, als\n",
    "X_diff = hstack((X_diff, tmp)).tocsr()\n",
    "\n",
    "\n",
    "wb = wordbatch.WordBatch(list2str, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0], \n",
    "                                                        \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 'binary',\n",
    "                                                        \"idf\": None}), procs=24)\n",
    "wb.dictionary_freeze= True\n",
    "tmp = wb.fit_transform(data['FileDiffgCust'])\n",
    "tmp = tmp[:, np.where(tmp.getnnz(axis=0) > 5)[0]]\n",
    "print('Shape of X_prod: {0}'.format(tmp.shape))\n",
    "# Fit ALS\n",
    "als = implicit.als.AlternatingLeastSquares(factors=50)\n",
    "als.fit(tmp)\n",
    "tmp = csr_matrix(als.item_factors)\n",
    "\n",
    "del wb, als\n",
    "X_diff = hstack((X_diff, tmp)).tocsr()\n",
    "\n",
    "wb = wordbatch.WordBatch(list2str, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0], \n",
    "                                                        \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 'binary',\n",
    "                                                        \"idf\": None}), procs=24)\n",
    "wb.dictionary_freeze= True\n",
    "tmp = wb.fit_transform(data['FileDiffgProd'])\n",
    "tmp = tmp[:, np.where(tmp.getnnz(axis=0) > 1)[0]]\n",
    "print('Shape of X_prod: {0}'.format(tmp.shape))\n",
    "# Fit ALS\n",
    "als = implicit.als.AlternatingLeastSquares(factors=50)\n",
    "als.fit(tmp)\n",
    "tmp = csr_matrix(als.item_factors)\n",
    "\n",
    "del wb, als\n",
    "X_diff = hstack((X_diff, tmp)).tocsr()\n",
    "\n",
    "data.drop(['FileDiff', 'FileDiffgFile', 'FileDiffgCust', 'FileDiffgProd'], axis=1, inplace=True)\n",
    "del tmp\n",
    "gc.collect()\n",
    "print('Vectorize `Time Difference` completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count FileID completed.\n",
      "Idiot count completed.\n"
     ]
    }
   ],
   "source": [
    "# Numbers of FileID\n",
    "tmp = log_data.groupby('FileID').apply(len)\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'FileCount']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Numbers of unique CustomerID\n",
    "tmp = log_data.groupby('FileID')['CustomerID'].apply(lambda x: len(np.unique(x)))\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'CustCount']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Numbers of unique ProductID\n",
    "tmp = log_data.groupby('FileID')['ProductID'].apply(lambda x: len(np.unique(x)))\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'ProdCount']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "print('Count FileID completed.')\n",
    "\n",
    "# Numbers of idiot countings from FileId and ProductId\n",
    "tmp = log_data.groupby('FileID')['CustomerID'].apply(lambda x: len(''.join(x)))\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'DigfCustLen']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "tmp = log_data.groupby('FileID')['CustomerID'].apply(lambda x: sum(c.isdigit() for c in ''.join(x))/len(''.join(x)))\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'DigfCustProp']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "tmp = log_data.groupby('FileID')['ProductID'].apply(lambda x: len(''.join(x)))\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'DigfProdLen']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "tmp = log_data.groupby('FileID')['ProductID'].apply(lambda x: sum(c.isdigit() for c in ''.join(x))/len(''.join(x)))\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'DigfProdProp']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "\n",
    "# Idiot counting\n",
    "data['NumDig'] = data['FileID'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "data['NumA'] = data['FileID'].apply(lambda x: x.count('a'))\n",
    "data['NumB'] = data['FileID'].apply(lambda x: x.count('b'))\n",
    "data['NumC'] = data['FileID'].apply(lambda x: x.count('c'))\n",
    "data['NumD'] = data['FileID'].apply(lambda x: x.count('d'))\n",
    "data['NumE'] = data['FileID'].apply(lambda x: x.count('e'))\n",
    "print('Idiot count completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date preprocessing\n",
    "log_data['Month'] = log_data['QueryTS'].dt.month\n",
    "log_data['Day'] = log_data['QueryTS'].dt.day\n",
    "log_data['Hour'] = log_data['QueryTS'].dt.hour\n",
    "log_data['Minute'] = log_data['QueryTS'].dt.minute\n",
    "log_data['Second'] = log_data['QueryTS'].dt.second\n",
    "log_data['WoY'] = log_data['QueryTS'].dt.weekofyear\n",
    "log_data['DoW'] = log_data['QueryTS'].dt.dayofweek\n",
    "log_data['DoY'] = log_data['QueryTS'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate datetime features completed.\n"
     ]
    }
   ],
   "source": [
    "# Dealing with datetime\n",
    "cols_name = ['Month', 'Day', 'Hour', 'Minute', 'Second', 'WoY', 'DoW', 'DoY']\n",
    "# Mean\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.mean)\n",
    "tmp.columns = [col+'_mean' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Ten percentile\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(lambda x: np.percentile(x, q=10))\n",
    "tmp.columns = [col+'_10per' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# First Quartile\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(lambda x: np.percentile(x, q=25))\n",
    "tmp.columns = [col+'_25per' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Third Quartile\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(lambda x: np.percentile(x, q=75))\n",
    "tmp.columns = [col+'_75per' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Ninety percentile\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(lambda x: np.percentile(x, q=90))\n",
    "tmp.columns = [col+'_90per' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Median\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.median)\n",
    "tmp.columns = [col+'_median' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Minimum\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.min)\n",
    "tmp.columns = [col+'_min' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Maximum\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.max)\n",
    "tmp.columns = [col+'_max' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# standard deviation\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.std)\n",
    "tmp.columns = [col+'_std' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "print('Generate datetime features completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate difference datetime features completed.\n"
     ]
    }
   ],
   "source": [
    "def dt_percentile(x, p):\n",
    "    index = range(len(x))\n",
    "    return x.iloc[np.int((np.percentile(index, p)))]    \n",
    "\n",
    "latest_date = log_data['QueryTS'].iloc[-1]\n",
    "\n",
    "# Minimum\n",
    "tmp = log_data.groupby('FileID')['QueryTS'].agg(lambda x: latest_date - np.min(x)).dt.total_seconds()\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'Duration_min']\n",
    "data = pd.merge(data, tmp.reset_index(drop=True), how='left', on='FileID')\n",
    "# Ten percentile\n",
    "tmp = log_data.groupby('FileID')['QueryTS'].agg(lambda x: latest_date - dt_percentile(x, 10)).dt.total_seconds()\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'Duration_10per']\n",
    "data = pd.merge(data, tmp.reset_index(drop=True), how='left', on='FileID')\n",
    "# First Quantile\n",
    "tmp = log_data.groupby('FileID')['QueryTS'].agg(lambda x: latest_date - dt_percentile(x, 25)).dt.total_seconds()\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'Duration_25per']\n",
    "data = pd.merge(data, tmp.reset_index(drop=True), how='left', on='FileID')\n",
    "# Median\n",
    "tmp = log_data.groupby('FileID')['QueryTS'].agg(lambda x: latest_date - dt_percentile(x, 50)).dt.total_seconds()\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'Duration_median']\n",
    "data = pd.merge(data, tmp.reset_index(drop=True), how='left', on='FileID')\n",
    "# Third Quantile\n",
    "tmp = log_data.groupby('FileID')['QueryTS'].agg(lambda x: latest_date - dt_percentile(x, 75)).dt.total_seconds()\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'Duration_75per']\n",
    "data = pd.merge(data, tmp.reset_index(drop=True), how='left', on='FileID')\n",
    "# Ninety percentile\n",
    "tmp = log_data.groupby('FileID')['QueryTS'].agg(lambda x: latest_date - dt_percentile(x, 90)).dt.total_seconds()\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'Duration_90per']\n",
    "data = pd.merge(data, tmp.reset_index(drop=True), how='left', on='FileID')\n",
    "# Maximum\n",
    "tmp = log_data.groupby('FileID')['QueryTS'].agg(lambda x: latest_date - np.max(x)).dt.total_seconds()\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'Duration_max']\n",
    "data = pd.merge(data, tmp.reset_index(drop=True), how='left', on='FileID')\n",
    "# Max - Min\n",
    "data['Duration_range'] = data['Duration_min'] - data['Duration_max']\n",
    "\n",
    "print('Generate difference datetime features completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime groupby completed.\n"
     ]
    }
   ],
   "source": [
    "# Count the difference time between each file was scaned\n",
    "cols_name = ['FileDiff', 'FileDiffgFile', 'FileDiffgCust', 'FileDiffgProd']\n",
    "log_data[cols_name] = log_data[cols_name].fillna(-1)\n",
    "# Mean\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.mean)\n",
    "tmp.columns = [col+'_mean' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp.reset_index(), how='left', on='FileID')\n",
    "# Ten percentile\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(lambda x: np.percentile(x, q=10))\n",
    "tmp.columns = [col+'_10per' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp.reset_index(), how='left', on='FileID')\n",
    "# First Quartile\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(lambda x: np.percentile(x, q=25))\n",
    "tmp.columns = [col+'_25per' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp.reset_index(), how='left', on='FileID')\n",
    "# Third Quartile\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(lambda x: np.percentile(x, q=75))\n",
    "tmp.columns = [col+'_75per' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp.reset_index(), how='left', on='FileID')\n",
    "# Ninety percentile\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(lambda x: np.percentile(x, q=90))\n",
    "tmp.columns = [col+'_90per' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp.reset_index(), how='left', on='FileID')\n",
    "# Median\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.median)\n",
    "tmp.columns = [col+'_median' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp.reset_index(), how='left', on='FileID')\n",
    "# Minimum\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.min)\n",
    "tmp.columns = [col+'_min' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp.reset_index(), how='left', on='FileID')\n",
    "# Maximum\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.max)\n",
    "tmp.columns = [col+'_max' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp.reset_index(), how='left', on='FileID')\n",
    "# standard deviation\n",
    "tmp = log_data.groupby('FileID')[cols_name].agg(np.std)\n",
    "tmp.columns = [col+'_std' for col in tmp.columns]\n",
    "tmp.reset_index(inplace=True)\n",
    "data = pd.merge(data, tmp.reset_index(), how='left', on='FileID')\n",
    "print('Datetime groupby completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or Load?\n",
    "if False:\n",
    "    save_sparse_csr('/disk/albert/Top1/als_{0}.npz'.format('v2'), X_als)\n",
    "    save_sparse_csr('/disk/albert/Top1/cust_{0}.npz'.format('v2'), X_cust)\n",
    "    save_sparse_csr('/disk/albert/Top1/prod_{0}.npz'.format('v2'), X_prod)\n",
    "    save_sparse_csr('/disk/albert/Top1/diff_{0}.npz'.format('v2'), X_diff)\n",
    "\n",
    "if True:\n",
    "    X_als = load_sparse_csr('/disk/albert/Top1/als_v2.npz')\n",
    "    X_cust = load_sparse_csr('/disk/albert/Top1/cust_v2.npz')\n",
    "    X_prod = load_sparse_csr('/disk/albert/Top1/prod_v2.npz')\n",
    "    X_diff = load_sparse_csr('/disk/albert/Top1/diff_v2.npz')\n",
    "    \n",
    "y = data['Target']\n",
    "cv_folds = data['Fold']\n",
    "cv_folds.dropna(inplace=True)\n",
    "y = y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Target']\n",
    "cv_folds = data['Fold']\n",
    "cv_folds.dropna(inplace=True)\n",
    "y = y.dropna()\n",
    "data.drop(['FileID', 'Target', 'Fold', 'index_x', 'index_y', 'index'], axis=1, inplace=True)\n",
    "tmp = data.std()\n",
    "col_std0 = tmp[tmp.isnull()].index\n",
    "data.drop(col_std0, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./backup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TargetCust_mean_mean</th>\n",
       "      <th>TargetCust_std_mean</th>\n",
       "      <th>TargetCust_sum_mean</th>\n",
       "      <th>TargetProd_mean_mean</th>\n",
       "      <th>TargetProd_std_mean</th>\n",
       "      <th>TargetProd_sum_mean</th>\n",
       "      <th>TargetCust_mean_std</th>\n",
       "      <th>TargetCust_std_std</th>\n",
       "      <th>TargetCust_sum_std</th>\n",
       "      <th>TargetProd_mean_std</th>\n",
       "      <th>...</th>\n",
       "      <th>FileDiffgCust_min</th>\n",
       "      <th>FileDiffgProd_min</th>\n",
       "      <th>FileDiff_max</th>\n",
       "      <th>FileDiffgFile_max</th>\n",
       "      <th>FileDiffgCust_max</th>\n",
       "      <th>FileDiffgProd_max</th>\n",
       "      <th>FileDiff_std</th>\n",
       "      <th>FileDiffgFile_std</th>\n",
       "      <th>FileDiffgCust_std</th>\n",
       "      <th>FileDiffgProd_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036583</td>\n",
       "      <td>0.187681</td>\n",
       "      <td>297715.297872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>287902.0</td>\n",
       "      <td>287902.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.397727</td>\n",
       "      <td>42758.474555</td>\n",
       "      <td>41977.666485</td>\n",
       "      <td>0.706125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>0.017495</td>\n",
       "      <td>0.127007</td>\n",
       "      <td>102680.264957</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.159554</td>\n",
       "      <td>0.009832</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22908.0</td>\n",
       "      <td>905563.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.325257</td>\n",
       "      <td>2883.826921</td>\n",
       "      <td>82068.297495</td>\n",
       "      <td>2.032187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.084151</td>\n",
       "      <td>0.162664</td>\n",
       "      <td>84610.500000</td>\n",
       "      <td>0.121472</td>\n",
       "      <td>0.298766</td>\n",
       "      <td>524881.817460</td>\n",
       "      <td>0.107129</td>\n",
       "      <td>0.202659</td>\n",
       "      <td>107714.216372</td>\n",
       "      <td>0.080458</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>56130.0</td>\n",
       "      <td>2965080.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.450269</td>\n",
       "      <td>5353.024683</td>\n",
       "      <td>209513.933912</td>\n",
       "      <td>4.161094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038890</td>\n",
       "      <td>0.073787</td>\n",
       "      <td>38485.081522</td>\n",
       "      <td>0.066022</td>\n",
       "      <td>0.219548</td>\n",
       "      <td>893258.858696</td>\n",
       "      <td>0.083666</td>\n",
       "      <td>0.158205</td>\n",
       "      <td>83990.385228</td>\n",
       "      <td>0.071581</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80033.0</td>\n",
       "      <td>2791885.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.227011</td>\n",
       "      <td>6874.729023</td>\n",
       "      <td>369618.759118</td>\n",
       "      <td>4.147249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.036833</td>\n",
       "      <td>0.072184</td>\n",
       "      <td>36560.821739</td>\n",
       "      <td>0.063297</td>\n",
       "      <td>0.216530</td>\n",
       "      <td>836606.008511</td>\n",
       "      <td>0.081888</td>\n",
       "      <td>0.156051</td>\n",
       "      <td>82360.790847</td>\n",
       "      <td>0.069086</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>62842.0</td>\n",
       "      <td>2774995.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>6694.117995</td>\n",
       "      <td>620568.905081</td>\n",
       "      <td>2.691969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TargetCust_mean_mean  TargetCust_std_mean  TargetCust_sum_mean  \\\n",
       "0              0.000000             0.000000             0.000000   \n",
       "1              0.000215             0.001605             0.017094   \n",
       "2              0.084151             0.162664         84610.500000   \n",
       "3              0.038890             0.073787         38485.081522   \n",
       "4              0.036833             0.072184         36560.821739   \n",
       "\n",
       "   TargetProd_mean_mean  TargetProd_std_mean  TargetProd_sum_mean  \\\n",
       "0              0.036583             0.187681        297715.297872   \n",
       "1              0.017495             0.127007        102680.264957   \n",
       "2              0.121472             0.298766        524881.817460   \n",
       "3              0.066022             0.219548        893258.858696   \n",
       "4              0.063297             0.216530        836606.008511   \n",
       "\n",
       "   TargetCust_mean_std  TargetCust_std_std  TargetCust_sum_std  \\\n",
       "0             0.000000            0.000000            0.000000   \n",
       "1             0.002068            0.014574            0.159554   \n",
       "2             0.107129            0.202659       107714.216372   \n",
       "3             0.083666            0.158205        83990.385228   \n",
       "4             0.081888            0.156051        82360.790847   \n",
       "\n",
       "   TargetProd_mean_std        ...          FileDiffgCust_min  \\\n",
       "0             0.001691        ...                       -1.0   \n",
       "1             0.009832        ...                       -1.0   \n",
       "2             0.080458        ...                       -1.0   \n",
       "3             0.071581        ...                       -1.0   \n",
       "4             0.069086        ...                       -1.0   \n",
       "\n",
       "   FileDiffgProd_min  FileDiff_max  FileDiffgFile_max  FileDiffgCust_max  \\\n",
       "0                0.0           1.0           287902.0           287902.0   \n",
       "1                0.0           1.0            22908.0           905563.0   \n",
       "2                0.0           3.0            56130.0          2965080.0   \n",
       "3                0.0           1.0            80033.0          2791885.0   \n",
       "4                0.0           2.0            62842.0          2774995.0   \n",
       "\n",
       "   FileDiffgProd_max  FileDiff_std  FileDiffgFile_std  FileDiffgCust_std  \\\n",
       "0                3.0      0.397727       42758.474555       41977.666485   \n",
       "1               26.0      0.325257        2883.826921       82068.297495   \n",
       "2               24.0      0.450269        5353.024683      209513.933912   \n",
       "3               35.0      0.227011        6874.729023      369618.759118   \n",
       "4               21.0      0.472868        6694.117995      620568.905081   \n",
       "\n",
       "   FileDiffgProd_std  \n",
       "0           0.706125  \n",
       "1           2.032187  \n",
       "2           4.161094  \n",
       "3           4.147249  \n",
       "4           2.691969  \n",
       "\n",
       "[5 rows x 187 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data.std()\n",
    "data.drop(tmp[tmp<=0].index, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['FileDiff', 'FileDiffgFile', 'FileDiffgCust', 'FileDiffgProd'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(lambda x: (x-np.nanmean(x))/(np.nanstd(x)+np.nanmean(x)+1))\n",
    "data.fillna(-1, inplace=True)\n",
    "X_data = csr_matrix(data)\n",
    "X_data = hstack((X_data, X_als, X_cust, X_prod, X_diff)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Target']\n",
    "cv_folds = data['Fold']\n",
    "cv_folds.dropna(inplace=True)\n",
    "y = y.dropna()\n",
    "data.drop(['FileID', 'Target', 'Fold'], axis=1, inplace=True)\n",
    "data.fillna(-1, inplace=True)\n",
    "data = data.apply(lambda x: (x-np.mean(x))/(np.std(x)+np.mean(x)+1))\n",
    "X_data = csr_matrix(data)\n",
    "save_sparse_csr('/disk/albert/Top1/data_{0}.npz'.format('v3'), X_data)\n",
    "X_data = hstack((X_data, X_als, X_cust, X_prod, X_diff)).tocsr()\n",
    "print('Shape of X_data: {0}'.format(X_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3001"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_sparse_csr('/disk/albert/Top1/new_process_{0}.npz'.format('v3'), X_data)\n",
    "del X_als, X_cust, X_prod, X_diff\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Final bagging answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'application': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': True,\n",
    "    \n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 3,\n",
    "    'feature_fraction': 0.66,\n",
    "    'max_depth': 10,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'num_leaves': 87,\n",
    "\n",
    "    'verbosity': -1,\n",
    "    'data_random_seed': 1,\n",
    "    'max_bin': 64,\n",
    "    'nthread': 18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM validation AUC: 0.977536\n",
      "LGBM validation AUC: 0.978124\n",
      "LGBM validation AUC: 0.977730\n",
      "LGBM validation AUC: 0.977607\n",
      "LGBM validation AUC: 0.977632\n",
      "==============================\n",
      "LGBM validation AUC: 0.978451\n",
      "LGBM validation AUC: 0.978161\n",
      "LGBM validation AUC: 0.977670\n",
      "LGBM validation AUC: 0.977853\n",
      "LGBM validation AUC: 0.978295\n",
      "==============================\n",
      "LGBM validation AUC: 0.973577\n",
      "LGBM validation AUC: 0.974007\n",
      "LGBM validation AUC: 0.974841\n",
      "LGBM validation AUC: 0.974382\n",
      "LGBM validation AUC: 0.974169\n",
      "==============================\n",
      "LGBM validation AUC: 0.980948\n",
      "LGBM validation AUC: 0.980867\n",
      "LGBM validation AUC: 0.980600\n",
      "LGBM validation AUC: 0.981171\n",
      "LGBM validation AUC: 0.980600\n",
      "==============================\n",
      "LGBM validation AUC: 0.980504\n",
      "LGBM validation AUC: 0.979798\n",
      "LGBM validation AUC: 0.980010\n",
      "LGBM validation AUC: 0.979869\n",
      "LGBM validation AUC: 0.979767\n",
      "==============================\n",
      "LGBM validation AUC: 0.981790\n",
      "LGBM validation AUC: 0.981440\n",
      "LGBM validation AUC: 0.981314\n",
      "LGBM validation AUC: 0.980964\n",
      "LGBM validation AUC: 0.981631\n",
      "==============================\n",
      "LGBM validation AUC: 0.980891\n",
      "LGBM validation AUC: 0.980374\n",
      "LGBM validation AUC: 0.980431\n",
      "LGBM validation AUC: 0.980061\n",
      "LGBM validation AUC: 0.980291\n",
      "==============================\n",
      "LGBM validation AUC: 0.979656\n",
      "LGBM validation AUC: 0.979279\n",
      "LGBM validation AUC: 0.979832\n",
      "LGBM validation AUC: 0.979896\n",
      "LGBM validation AUC: 0.980215\n",
      "==============================\n",
      "LGBM validation AUC: 0.977461\n",
      "LGBM validation AUC: 0.977063\n",
      "LGBM validation AUC: 0.976820\n",
      "LGBM validation AUC: 0.977419\n",
      "LGBM validation AUC: 0.977044\n",
      "==============================\n",
      "LGBM validation AUC: 0.980100\n",
      "LGBM validation AUC: 0.980124\n",
      "LGBM validation AUC: 0.979988\n",
      "LGBM validation AUC: 0.979974\n",
      "LGBM validation AUC: 0.980285\n",
      "==============================\n",
      "Predict LGBM completed.\n"
     ]
    }
   ],
   "source": [
    "# Reset data\n",
    "train_X = X_data[:len(train_set)]\n",
    "test_X = X_data[len(train_set):]\n",
    "train_y = y[:len(train_set)]\n",
    "\n",
    "n_bags = 5\n",
    "predsL = 0\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    train_idx, valid_idx = cv_folds.loc[cv_folds != fold].index, cv_folds.loc[cv_folds == fold].index\n",
    "    \n",
    "    d_train = lgb.Dataset(train_X[train_idx], label=train_y[train_idx])\n",
    "    d_valid = lgb.Dataset(train_X[valid_idx], label=train_y[valid_idx])\n",
    "    watchlist = [d_train, d_valid]\n",
    "    \n",
    "    for bgs in range(n_bags):\n",
    "        params['feature_fraction_seed'] = np.random.randint(10000)\n",
    "        params['bagging_seed'] = np.random.randint(10000)\n",
    "    \n",
    "        model = lgb.train(params, train_set=d_train, num_boost_round=2600, valid_sets=watchlist, \n",
    "                          early_stopping_rounds=200, verbose_eval=False)\n",
    "        \n",
    "        tmpL = model.predict(train_X[valid_idx])\n",
    "        tmpL = (tmpL - min(tmpL))/(max(tmpL) - min(tmpL))\n",
    "        print(\"LGBM validation AUC: {0:.6f}\".format(roc_auc_score(train_y[valid_idx], tmpL)))\n",
    "        predsL += model.predict(test_X)\n",
    "    print('='*30)\n",
    "\n",
    "predsL /= (n_folds*n_bags)\n",
    "predsL = (predsL - min(predsL))/(max(predsL) - min(predsL))\n",
    "print('Predict LGBM completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.concat([test_set[['FileID']], pd.Series(predsL)], axis=1)\n",
    "submit.columns = ['FileID', 'Probability']\n",
    "submit.to_csv('./LGBM_{0}bag_{1}.csv'.format(n_bags*n_folds, re.sub('-', '', str(datetime.date.today())[5:])),\n",
    "              index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
