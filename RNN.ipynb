{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FM_FTRL\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, GlobalAveragePooling1D, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>QueryTS</th>\n",
       "      <th>ProductID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77043f410ba7dc8dced745db94a1fcba</td>\n",
       "      <td>02142c5dccec42262a1e88dc8416f5b7</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>634e6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c5ad385cba5ac43b0263f3d804c8f823</td>\n",
       "      <td>8d7a1059ef78672047e45cf1b84d6276</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>7acab3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c5ad385cba5ac43b0263f3d804c8f823</td>\n",
       "      <td>8d7a1059ef78672047e45cf1b84d6276</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>7acab3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75d7a7507ac89b33191b18d56af7544b</td>\n",
       "      <td>8d7a1059ef78672047e45cf1b84d6276</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>7acab3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75d7a7507ac89b33191b18d56af7544b</td>\n",
       "      <td>8d7a1059ef78672047e45cf1b84d6276</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>7acab3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FileID                        CustomerID  \\\n",
       "0  77043f410ba7dc8dced745db94a1fcba  02142c5dccec42262a1e88dc8416f5b7   \n",
       "1  c5ad385cba5ac43b0263f3d804c8f823  8d7a1059ef78672047e45cf1b84d6276   \n",
       "2  c5ad385cba5ac43b0263f3d804c8f823  8d7a1059ef78672047e45cf1b84d6276   \n",
       "3  75d7a7507ac89b33191b18d56af7544b  8d7a1059ef78672047e45cf1b84d6276   \n",
       "4  75d7a7507ac89b33191b18d56af7544b  8d7a1059ef78672047e45cf1b84d6276   \n",
       "\n",
       "     QueryTS ProductID  \n",
       "0 2017-03-01    634e6b  \n",
       "1 2017-03-01    7acab3  \n",
       "2 2017-03-01    7acab3  \n",
       "3 2017-03-01    7acab3  \n",
       "4 2017-03-01    7acab3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = '/disk/Tbrain/'\n",
    "\n",
    "train_set = pd.read_csv(dir_path+'training-set.csv', header=None, names=['FileID', 'Target'])\n",
    "test_set = pd.read_csv(dir_path+'testing-set.csv', header=None, names=['FileID', 'Target'])\n",
    "train_ex = pd.read_table(dir_path+'exception/exception_train.txt', header=None, names=['FileID'])\n",
    "test_ex = pd.read_table(dir_path+'exception/exception_testing.txt', header=None, names=['FileID'])\n",
    "\n",
    "train_set = train_set.loc[~train_set['FileID'].isin(train_ex)]\n",
    "test_set = test_set.loc[~test_set['FileID'].isin(test_ex)]\n",
    "\n",
    "\n",
    "log_data = pd.read_csv(dir_path+'log_data.csv')\n",
    "log_data['QueryTS'] = pd.to_datetime(log_data['QueryTS'], format='%Y-%m-%d %H:%M:%S')\n",
    "log_data.sort_values(['QueryTS'], ascending=True, na_position='first', inplace=True)\n",
    "log_data.reset_index(drop=True, inplace=True)\n",
    "log_data['ProductID'] = log_data['ProductID'].astype(str)\n",
    "print('Load data complete.')\n",
    "\n",
    "del train_ex, test_ex\n",
    "gc.collect()\n",
    "\n",
    "log_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e2398b12121a85166fed5fe2a3da</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001fe8dce14ce099aa6ca8ea5026ea7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00027f50019000accc492e5684efc818</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00028c9da3573ec50db74b44310ae507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0003dc8130969abe688cadf5f14ea19f</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             FileID  Target\n",
       "0  0000e2398b12121a85166fed5fe2a3da     0.0\n",
       "1  0001fe8dce14ce099aa6ca8ea5026ea7     0.0\n",
       "2  00027f50019000accc492e5684efc818     0.0\n",
       "3  00028c9da3573ec50db74b44310ae507     0.0\n",
       "4  0003dc8130969abe688cadf5f14ea19f     0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([train_set, test_set],axis=0)\n",
    "data['Target'].replace(0.5, np.nan, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentail lebal encoding completed.\n",
      "Count FileID completed.\n",
      "Idiot count completed.\n",
      "Datetime sequentail completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoding\n",
    "log_data['CustomerID'] = log_data['CustomerID'].astype('category').cat.codes\n",
    "log_data['ProductID'] = log_data['ProductID'].astype('category').cat.codes\n",
    "# Sequence of CustomerID\n",
    "tmp = log_data.groupby('FileID')['CustomerID'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "# Sequence of ProductID\n",
    "tmp = log_data.groupby('FileID')['ProductID'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "print('Sequentail lebal encoding completed.')\n",
    "\n",
    "# Numbers of FileID\n",
    "tmp = log_data.groupby('FileID').apply(len)\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'FileCount']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Numbers of unique CustomerID\n",
    "tmp = log_data.groupby('FileID')['CustomerID'].apply(lambda x: len(np.unique(x)))\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'CustCount']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "# Numbers of unique ProductID\n",
    "tmp = log_data.groupby('FileID')['ProductID'].apply(lambda x: len(np.unique(x)))\n",
    "tmp = tmp.to_frame().reset_index()\n",
    "tmp.columns = ['FileID', 'ProdCount']\n",
    "data = pd.merge(data, tmp, how='left', on='FileID')\n",
    "print('Count FileID completed.')\n",
    "\n",
    "# Idiot counting\n",
    "data['NumDig'] = data['FileID'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "data['NumA'] = data['FileID'].apply(lambda x: x.count('a'))\n",
    "data['NumB'] = data['FileID'].apply(lambda x: x.count('b'))\n",
    "data['NumC'] = data['FileID'].apply(lambda x: x.count('c'))\n",
    "data['NumD'] = data['FileID'].apply(lambda x: x.count('d'))\n",
    "data['NumE'] = data['FileID'].apply(lambda x: x.count('e'))\n",
    "print('Idiot count completed.')\n",
    "\n",
    "# Dealing with datetime\n",
    "log_data['FileDiff'] = log_data['QueryTS'].diff().dt.total_seconds()\n",
    "log_data['FileDiffgCust'] = log_data.groupby('CustomerID')['QueryTS'].diff().dt.total_seconds()\n",
    "log_data.fillna(-1, inplace=True)\n",
    "# -1 will get fail from Keras\n",
    "log_data['FileDiff'] = log_data['FileDiff'].astype('category').cat.codes\n",
    "log_data['FileDiffgCust'] = log_data['FileDiffgCust'].astype('category').cat.codes\n",
    "\n",
    "# Sequence of FileDiff\n",
    "tmp = log_data.groupby('FileID')['FileDiff'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "# Sequence of FileDiffgCust\n",
    "tmp = log_data.groupby('FileID')['FileDiffgCust'].apply(list)\n",
    "data = pd.merge(data, tmp.to_frame().reset_index(), how='left', on='FileID')\n",
    "print('Datetime sequentail completed.')\n",
    "\n",
    "del tmp, log_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data\n",
    "# data.to_csv('/disk/albert/Top1/RNN_preproc.csv', index=False)\n",
    "\n",
    "# data = pd.read_csv('/disk/albert/Top1/RNN_preproc.csv')\n",
    "# for col in ['CustomerID', 'ProductID', 'FileDiff', 'FileDiffgCust']:\n",
    "#     data[col] = data[col].apply(literal_eval)\n",
    "    \n",
    "# New Features\n",
    "new_features = pd.read_csv('./backup.csv', usecols=['TargetCust_mean_mean', 'TargetCust_std_mean', \n",
    "                                                    'TargetCust_mean_std', 'TargetCust_std_std'])\n",
    "\n",
    "data = pd.concat([data, new_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ = 500\n",
    "\n",
    "MAX_CUST = np.max(data['CustomerID'].apply(max)) + 1\n",
    "MAX_PROD = np.max(data['ProductID'].apply(max)) + 1\n",
    "MAX_TIME = int(np.max([data['FileDiff'].apply(max), \n",
    "                       data['FileDiffgCust'].apply(max)])) + 2\n",
    "MAX_FILE = int(np.max([data['FileCount'].max(), \n",
    "                       data['CustCount'].max(), \n",
    "                       data['ProdCount'].max()])) + 2\n",
    "MAX_COUT = np.max([data['NumDig'].max(), \n",
    "                   data['NumA'].max(), \n",
    "                   data['NumB'].max(),\n",
    "                   data['NumC'].max(),\n",
    "                   data['NumD'].max(),\n",
    "                   data['NumE'].max()]) + 1\n",
    "\n",
    "def get_keras_data(df):\n",
    "    X = {\n",
    "        'customerid': pad_sequences(df['CustomerID'], maxlen=MAX_SEQ),\n",
    "        'productid': pad_sequences(df['ProductID'], maxlen=MAX_SEQ),\n",
    "        'filediffgcust': pad_sequences(df['FileDiffgCust'], maxlen=MAX_SEQ),\n",
    "        'TargetCust_mm': np.array(df[['TargetCust_mean_mean']]),\n",
    "        'TargetCust_ms': np.array(df[['TargetCust_mean_std']]),\n",
    "        'TargetCust_sm': np.array(df[['TargetCust_std_mean']]),\n",
    "        'TargetCust_ss': np.array(df[['TargetCust_std_std']]),\n",
    "        'file_count': np.array(df[[\"FileCount\"]]),\n",
    "        'cust_count': np.array(df[[\"CustCount\"]]),\n",
    "        'prod_count': np.array(df[[\"ProdCount\"]]),  \n",
    "        'num_dig': np.array(df[[\"NumDig\"]]),\n",
    "        'num_a': np.array(df[[\"NumA\"]]),\n",
    "        'num_b': np.array(df[[\"NumB\"]]),\n",
    "        'num_c': np.array(df[[\"NumC\"]]),\n",
    "        'num_d': np.array(df[[\"NumD\"]]),\n",
    "        'num_e': np.array(df[[\"NumE\"]]),\n",
    "    }\n",
    "    return X\n",
    "\n",
    "\n",
    "def rnn_model(df, lr=0.001, decay=0.0):    \n",
    "    # Inputs\n",
    "    customerid = Input(shape=[df['customerid'].shape[1]], name='customerid')\n",
    "    productid = Input(shape=[df['productid'].shape[1]], name='productid')\n",
    "    filediffgcust = Input(shape=[df['filediffgcust'].shape[1]], name='filediffgcust')\n",
    "    file_count = Input(shape=[1], name='file_count')\n",
    "    cust_count = Input(shape=[1], name='cust_count')\n",
    "    prod_count = Input(shape=[1], name='prod_count')\n",
    "    num_dig = Input(shape=[1], name='num_dig')\n",
    "    num_a = Input(shape=[1], name='num_a')\n",
    "    num_b = Input(shape=[1], name='num_b')\n",
    "    num_c = Input(shape=[1], name='num_c')\n",
    "    num_d = Input(shape=[1], name='num_d')\n",
    "    num_e = Input(shape=[1], name='num_e')\n",
    "    TargetCust_mm = Input(shape=[1], name='TargetCust_mm')\n",
    "    TargetCust_ms = Input(shape=[1], name='TargetCust_ms')\n",
    "    TargetCust_sm = Input(shape=[1], name='TargetCust_sm')\n",
    "    TargetCust_ss = Input(shape=[1], name='TargetCust_ss')\n",
    "\n",
    "    # Embeddings layers\n",
    "    emb_customerid = Embedding(MAX_CUST, 80)(customerid)\n",
    "    emb_productid = Embedding(MAX_PROD, 5)(productid)\n",
    "    emb_filediffgcust = Embedding(MAX_TIME, 20)(filediffgcust)\n",
    "    emb_file_count = Embedding(MAX_FILE, 5)(file_count)\n",
    "    emb_cust_count = Embedding(MAX_FILE, 5)(cust_count)\n",
    "    emb_prod_count = Embedding(MAX_FILE, 5)(prod_count)\n",
    "    emb_dig_count = Embedding(MAX_COUT, 3)(num_dig)\n",
    "    emb_a_count = Embedding(MAX_COUT, 3)(num_a)\n",
    "    emb_b_count = Embedding(MAX_COUT, 3)(num_b)\n",
    "    emb_c_count = Embedding(MAX_COUT, 3)(num_c)\n",
    "    emb_d_count = Embedding(MAX_COUT, 3)(num_d)\n",
    "    emb_e_count = Embedding(MAX_COUT, 3)(num_e)\n",
    "\n",
    "    # rnn layers\n",
    "    rnn_layer1 = GRU(32)(emb_customerid)\n",
    "    rnn_layer2 = GRU(4)(emb_productid)\n",
    "    rnn_layer3 = GRU(8)(emb_filediffgcust)\n",
    "    \n",
    "    # FastText\n",
    "    fast_layer1 = GlobalAveragePooling1D()(emb_customerid)\n",
    "    fast_layer2 = GlobalAveragePooling1D()(emb_productid)\n",
    "    fast_layer3 = GlobalAveragePooling1D()(emb_filediffgcust)\n",
    "    \n",
    "    # main layers\n",
    "    main_l = concatenate([\n",
    "        TargetCust_mm,\n",
    "        TargetCust_ms,\n",
    "        TargetCust_sm,\n",
    "        TargetCust_ss,\n",
    "        Flatten()(emb_file_count),\n",
    "        Flatten()(emb_cust_count),\n",
    "        Flatten()(emb_prod_count),\n",
    "        Flatten()(emb_dig_count),\n",
    "        Flatten()(emb_a_count),\n",
    "        Flatten()(emb_b_count),\n",
    "        Flatten()(emb_c_count),\n",
    "        Flatten()(emb_d_count),        \n",
    "        Flatten()(emb_e_count),        \n",
    "        fast_layer1,\n",
    "        fast_layer2,\n",
    "        fast_layer3,\n",
    "        rnn_layer1,\n",
    "        rnn_layer2,\n",
    "        rnn_layer3,\n",
    "    ])\n",
    "\n",
    "    main_l = Dropout(0.3) (Dense(1024)(main_l))\n",
    "    main_l = BatchNormalization()(main_l)\n",
    "    main_l = Activation('elu')(main_l)\n",
    "\n",
    "    main_l = Dropout(0.2) (Dense(32)(main_l))\n",
    "    main_l = Activation('elu')(main_l)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\") (main_l)\n",
    "    model = Model([customerid, productid, filediffgcust, file_count, cust_count, prod_count, \n",
    "                   TargetCust_mm, TargetCust_ms, TargetCust_sm, TargetCust_ss,\n",
    "                   num_dig, num_a, num_b, num_c, num_d, num_e], output)\n",
    "\n",
    "    optimizer = Adam(lr=lr, decay=decay)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Final bagging answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data.loc[data['FileID'].isin(train_set['FileID'])].drop(['Target'], axis=1)\n",
    "train_y = data['Target'].loc[data['FileID'].isin(train_set['FileID'])]\n",
    "test_X = data.loc[~data['FileID'].isin(train_set['FileID'])].drop(['Target'], axis=1)\n",
    "test_X = get_keras_data(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN validation AUC: 0.937267\n",
      "RNN validation AUC: 0.942206\n",
      "RNN validation AUC: 0.924964\n",
      "RNN validation AUC: 0.936005\n",
      "RNN validation AUC: 0.943469\n",
      "RNN validation AUC: 0.938660\n",
      "RNN validation AUC: 0.939801\n",
      "RNN validation AUC: 0.930758\n",
      "RNN validation AUC: 0.941464\n",
      "RNN validation AUC: 0.952069\n",
      "RNN validation AUC: 0.939736\n",
      "RNN validation AUC: 0.935774\n",
      "RNN validation AUC: 0.942577\n",
      "RNN validation AUC: 0.905205\n",
      "RNN validation AUC: 0.955196\n",
      "RNN validation AUC: 0.949372\n",
      "RNN validation AUC: 0.951007\n",
      "RNN validation AUC: 0.951450\n",
      "RNN validation AUC: 0.942154\n",
      "RNN validation AUC: 0.945299\n",
      "RNN validation AUC: 0.935277\n",
      "RNN validation AUC: 0.920681\n",
      "RNN validation AUC: 0.942288\n",
      "RNN validation AUC: 0.941304\n",
      "RNN validation AUC: 0.938506\n",
      "RNN validation AUC: 0.927319\n"
     ]
    }
   ],
   "source": [
    "test_X = get_keras_data(test_X)\n",
    "\n",
    "n_folds = 10\n",
    "n_bags = 3\n",
    "nrow_train = len(train_set)\n",
    "skf = StratifiedKFold(n_splits=n_folds, random_state=5566)\n",
    "data['Fold'] = np.nan\n",
    "for f, (_, valid_idx) in enumerate(skf.split(data['FileID'].iloc[:nrow_train], data['Target'].iloc[:nrow_train])):\n",
    "    data['Fold'].iloc[valid_idx] = f\n",
    "\n",
    "y = data['Target']\n",
    "y = y.dropna()\n",
    "cv_folds = data['Fold'].dropna()\n",
    "\n",
    "train_X = data[:nrow_train]\n",
    "test_X = data[nrow_train:]\n",
    "train_y = y[:nrow_train]\n",
    "\n",
    "\n",
    "# Set hyper parameters for the model.\n",
    "BATCH_SIZE = 2**9\n",
    "epochs = 2\n",
    "predsR = 0\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    train_idx, valid_idx = cv_folds.loc[cv_folds != fold].index, cv_folds.loc[cv_folds == fold].index\n",
    "    X_train, y_train = get_keras_data(train_X.iloc[train_idx]), train_y.iloc[train_idx]\n",
    "    X_valid, y_valid = get_keras_data(train_X.iloc[valid_idx]), train_y.iloc[valid_idx]\n",
    "    \n",
    "    # Calculate learning rate decay.\n",
    "    exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "    nrow_train = len(y_train)\n",
    "    steps = int(nrow_train / BATCH_SIZE) * epochs\n",
    "    lr_init, lr_fin = 0.018, 0.0006\n",
    "    lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "\n",
    "    for bag in range(n_bags):\n",
    "        # Train model\n",
    "        model = rnn_model(df=X_train, lr=lr_init, decay=lr_decay)\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=BATCH_SIZE, verbose=False)\n",
    "        tmpR = model.predict(X_valid, batch_size=BATCH_SIZE)\n",
    "        predsR += model.predict(test_X, batch_size=BATCH_SIZE).squeeze()\n",
    "        \n",
    "        print(\"RNN validation AUC: {0:.6f}\".format(roc_auc_score(y_valid, tmpR)))\n",
    "        # Clearing session\n",
    "        K.clear_session()\n",
    "    print('='*30)\n",
    "    \n",
    "predsR /= (n_folds*n_bags)\n",
    "print('Predict RNN completed.')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.concat([test_set[['FileID']], pd.Series(predsR)], axis=1)\n",
    "submit.columns = ['FileID', 'Probability']\n",
    "submit.to_csv('./RNN_{0}bag_{1}.csv'.format(n_bags*n_folds, re.sub('-', '', str(datetime.date.today())[5:])),\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
