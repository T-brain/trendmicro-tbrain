{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Dense, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']),\n",
    "                      shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = '/disk/Tbrain/'\n",
    "\n",
    "train_set = pd.read_csv(dir_path+'training-set.csv', header=None, names=['FileID', 'Target'])\n",
    "test_set = pd.read_csv(dir_path+'testing-set.csv', header=None, names=['FileID', 'Target'])\n",
    "train_ex = pd.read_table(dir_path+'exception/exception_train.txt', header=None, names=['FileID'])\n",
    "test_ex = pd.read_table(dir_path+'exception/exception_testing.txt', header=None, names=['FileID'])\n",
    "\n",
    "train_set = train_set.loc[~train_set['FileID'].isin(train_ex)]\n",
    "test_set = test_set.loc[~test_set['FileID'].isin(test_ex)]\n",
    "\n",
    "data = pd.concat([train_set, test_set],axis=0)\n",
    "data['Target'].replace(0.5, np.nan, inplace=True)\n",
    "\n",
    "# Given fold number for each FileID (Need for feature engineering with y, prevent for overfitting)\n",
    "nrow_train = len(train_set)\n",
    "n_folds = 10\n",
    "skf = StratifiedKFold(n_splits=n_folds, random_state=5566)\n",
    "data['Fold'] = np.nan\n",
    "for f, (_, valid_idx) in enumerate(skf.split(data['FileID'].iloc[:nrow_train], data['Target'].iloc[:nrow_train])):\n",
    "    data['Fold'].iloc[valid_idx] = f\n",
    "\n",
    "cv_folds = data['Fold']\n",
    "cv_folds.dropna(inplace=True)\n",
    "y = data['Target']\n",
    "y = y.dropna()\n",
    "\n",
    "X_data = load_sparse_csr('/disk/albert/Top1/new_process_v3.npz')\n",
    "\n",
    "\n",
    "del train_ex, test_ex, data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_data = pd.DataFrame(X_data.toarray())\n",
    "tmp = X_data.isnull().sum()\n",
    "X_data.drop(tmp[tmp>0].index.tolist(), axis=1, inplace=True)\n",
    "X_data = csr_matrix(X_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NN model\n",
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4096, activation='tanh', input_dim=train_X.shape[1]))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(512, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))    \n",
    "    model.add(Dense(128, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Final bagging answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN validation AUC: 0.966674\n",
      "NN validation AUC: 0.967656\n",
      "NN validation AUC: 0.967792\n",
      "NN validation AUC: 0.968849\n",
      "NN validation AUC: 0.968289\n",
      "NN validation AUC: 0.965319\n",
      "NN validation AUC: 0.967960\n",
      "NN validation AUC: 0.967596\n",
      "NN validation AUC: 0.967455\n",
      "NN validation AUC: 0.968366\n",
      "==============================\n",
      "NN validation AUC: 0.962891\n",
      "NN validation AUC: 0.961262\n",
      "NN validation AUC: 0.964137\n",
      "NN validation AUC: 0.963599\n",
      "NN validation AUC: 0.963614\n",
      "NN validation AUC: 0.966314\n",
      "NN validation AUC: 0.966764\n",
      "NN validation AUC: 0.961669\n",
      "NN validation AUC: 0.964231\n",
      "NN validation AUC: 0.961384\n",
      "==============================\n",
      "NN validation AUC: 0.962729\n",
      "NN validation AUC: 0.963994\n",
      "NN validation AUC: 0.959669\n",
      "NN validation AUC: 0.961318\n",
      "NN validation AUC: 0.960211\n",
      "NN validation AUC: 0.963135\n",
      "NN validation AUC: 0.959233\n",
      "NN validation AUC: 0.961692\n",
      "NN validation AUC: 0.962204\n",
      "NN validation AUC: 0.961616\n",
      "==============================\n",
      "NN validation AUC: 0.971178\n",
      "NN validation AUC: 0.970822\n",
      "NN validation AUC: 0.970806\n",
      "NN validation AUC: 0.970733\n",
      "NN validation AUC: 0.970908\n",
      "NN validation AUC: 0.970073\n",
      "NN validation AUC: 0.968097\n",
      "NN validation AUC: 0.968041\n",
      "NN validation AUC: 0.969701\n",
      "NN validation AUC: 0.972492\n",
      "==============================\n",
      "NN validation AUC: 0.969461\n",
      "NN validation AUC: 0.967135\n",
      "NN validation AUC: 0.968271\n",
      "NN validation AUC: 0.968756\n",
      "NN validation AUC: 0.967551\n",
      "NN validation AUC: 0.968903\n",
      "NN validation AUC: 0.969959\n",
      "NN validation AUC: 0.965950\n",
      "NN validation AUC: 0.969914\n",
      "NN validation AUC: 0.971007\n",
      "==============================\n",
      "NN validation AUC: 0.973355\n",
      "NN validation AUC: 0.969853\n",
      "NN validation AUC: 0.973771\n",
      "NN validation AUC: 0.965476\n",
      "NN validation AUC: 0.969402\n",
      "NN validation AUC: 0.971661\n",
      "NN validation AUC: 0.971107\n",
      "NN validation AUC: 0.967520\n",
      "NN validation AUC: 0.970889\n",
      "NN validation AUC: 0.973137\n",
      "==============================\n",
      "NN validation AUC: 0.970280\n",
      "NN validation AUC: 0.972178\n",
      "NN validation AUC: 0.971836\n",
      "NN validation AUC: 0.968386\n",
      "NN validation AUC: 0.973023\n",
      "NN validation AUC: 0.969382\n",
      "NN validation AUC: 0.971162\n",
      "NN validation AUC: 0.967039\n",
      "NN validation AUC: 0.969983\n",
      "NN validation AUC: 0.972068\n",
      "==============================\n",
      "NN validation AUC: 0.967924\n",
      "NN validation AUC: 0.962963\n",
      "NN validation AUC: 0.965955\n",
      "NN validation AUC: 0.966430\n",
      "NN validation AUC: 0.967444\n",
      "NN validation AUC: 0.967606\n",
      "NN validation AUC: 0.962959\n",
      "NN validation AUC: 0.967563\n",
      "NN validation AUC: 0.969174\n",
      "NN validation AUC: 0.969121\n",
      "==============================\n",
      "NN validation AUC: 0.967374\n",
      "NN validation AUC: 0.966825\n",
      "NN validation AUC: 0.966503\n",
      "NN validation AUC: 0.966198\n",
      "NN validation AUC: 0.963232\n",
      "NN validation AUC: 0.970159\n",
      "NN validation AUC: 0.964010\n",
      "NN validation AUC: 0.968390\n",
      "NN validation AUC: 0.967018\n",
      "NN validation AUC: 0.963753\n",
      "==============================\n",
      "NN validation AUC: 0.967117\n",
      "NN validation AUC: 0.971260\n",
      "NN validation AUC: 0.966796\n",
      "NN validation AUC: 0.966754\n",
      "NN validation AUC: 0.969167\n",
      "NN validation AUC: 0.967876\n",
      "NN validation AUC: 0.967282\n",
      "NN validation AUC: 0.968554\n",
      "NN validation AUC: 0.969330\n",
      "NN validation AUC: 0.969319\n",
      "==============================\n",
      "Predict NN completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21210"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset data\n",
    "train_X = X_data[:len(train_set)]\n",
    "test_X = X_data[len(train_set):]\n",
    "train_y = y[:len(train_set)]\n",
    "\n",
    "n_bags = 10\n",
    "predsN = 0\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    train_idx, valid_idx = cv_folds.loc[cv_folds != fold].index, cv_folds.loc[cv_folds == fold].index\n",
    "\n",
    "    X_train, y_train = train_X[train_idx], train_y.iloc[train_idx]\n",
    "    X_valid, y_valid = train_X[valid_idx], train_y.iloc[valid_idx]\n",
    "    \n",
    "    for bag in range(n_bags):\n",
    "        # Train model\n",
    "        model = nn_model()\n",
    "        model.fit(X_train, y_train, batch_size=256, epochs=10, class_weight='auto', verbose=0)\n",
    "        tmpN = model.predict_proba(X_valid).squeeze()\n",
    "        predsN += model.predict_proba(test_X).squeeze()\n",
    "        \n",
    "        print(\"NN validation AUC: {0:.6f}\".format(roc_auc_score(y_valid, tmpN)))\n",
    "        # Clearing session\n",
    "        K.clear_session()\n",
    "    print('='*30)\n",
    "\n",
    "predsN /= (n_folds*n_bags)\n",
    "print('Predict NN completed.')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.concat([test_set[['FileID']], pd.Series(predsN)], axis=1)\n",
    "submit.columns = ['FileID', 'Probability']\n",
    "submit.to_csv('./NN_{0}bag_{1}.csv'.format(n_bags*n_folds, re.sub('-', '', str(datetime.date.today())[5:])),\n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
